{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sf3FGX2DBQ7Ijun0RO-NBkudD-sQQ2K0",
      "authorship_tag": "ABX9TyOZ7TqZgnK1HAn1ivp4f2iI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/affan1317/GEOL0069-EOYA/blob/main/Fetching_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source of data\n",
        "We are utilising the Copernicus Data Space together with Google Earth Engine, which provide extensive satellite imageries and geospatial data. It is also freely accessible, fostering open science and research. Google Earth Engine can be accessed with the normal Google account most people would already have, while The Copernicus Data Space needs a separate registration, which can be done via the [website](https://dataspace.copernicus.eu/)."
      ],
      "metadata": {
        "id": "uwOOJf5FhS3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we mount our Google Drive to allow easy and direct access to our files. Again, by using Google Colab together with Google Drive, local memory and storage would not be the limiting factor."
      ],
      "metadata": {
        "id": "iHd0a9oMKJO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHiLGDk-CLSX",
        "outputId": "7901a6cb-2327-456a-cebd-b5f5316b7af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Fetching Logic\n",
        "The process of data fetching is meticulously structured to ensure we efficiently retrieve accurate and comprehensive data. The logic underlying this process involves several key steps:\n",
        "\n",
        "1. Area and Time Specification: Initially, we define the geographical scope and the specific time frame of interest. This precise specification allows us to target our data retrieval effectively.\n",
        "\n",
        "2. Retrieving File Names from Google Earth Engine: Once the area and time parameters are set, we proceed to fetch a list of relevant file names from Google Earth Engine. This platform provides a robust interface for identifying datasets that match our specified criteria.\n",
        "\n",
        "3. Converting to Copernicus Filename Format: After obtaining the list of file names from Google Earth Engine, the next step involves transforming these names into the format recognised by the Copernicus Dataspace. This conversion is crucial for ensuring compatibility and facilitating the subsequent data retrieval process.\n",
        "\n",
        "4. Fetching Raw Data from Copernicus Dataspace: With the correctly formatted file names in hand, we then access the Copernicus Dataspace to retrieve the raw data. It's important to note that Google Earth Engine does not provide raw data in the same format as the Copernicus Dataspace. Hence, this step is essential for obtaining the data in its native, unaltered format."
      ],
      "metadata": {
        "id": "UDVf-Zlkl5sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial set up\n",
        "*   Install the required packages\n",
        "*   Authenticate with Google Earth\n",
        "\n"
      ],
      "metadata": {
        "id": "hOKdN4dcKHtS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BhhWMFdUwVX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from shapely.geometry import Polygon, Point\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import subprocess\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project = 'geol0069week3fetchingdata')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in the functions needed including:\n",
        "* Parsing GEE filename for Sentinel-2 and Sentinel-3\n",
        "* Quering Sentinel-2 and Sentinel-3 data based on extracted info\n",
        "* Converting GEE filename to Copernicus filename\n",
        "* Retrieving access token for Copernicus Data Space\n",
        "* Downloading the queried data"
      ],
      "metadata": {
        "id": "9z0AFny8KFaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebp3EAATUwVY"
      },
      "outputs": [],
      "source": [
        "def parse_gee_filename_s2(gee_filename):\n",
        "    \"\"\"\n",
        "    Parses the Google Earth Engine filename to extract satellite name, sensing date, and start time.\n",
        "\n",
        "    Parameters:\n",
        "    gee_filename (str): Filename obtained from Google Earth Engine.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains satellite name, sensing date, and start time.\n",
        "    \"\"\"\n",
        "    parts = gee_filename.split('_')\n",
        "    sensing_date = parts[0]\n",
        "    tile_number = parts[2]\n",
        "    return sensing_date, tile_number\n",
        "\n",
        "def parse_gee_filename_s3(gee_filename):\n",
        "    \"\"\"\n",
        "    Parses the Google Earth Engine filename to extract satellite name, sensing date, and start time.\n",
        "\n",
        "    Parameters:\n",
        "    gee_filename (str): Filename obtained from Google Earth Engine.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains satellite name, sensing date, and start time.\n",
        "    \"\"\"\n",
        "    parts = gee_filename.split('_')\n",
        "    satellite = parts[0] + '_OL_1_EFR'\n",
        "    start_datetime = parts[1]\n",
        "    end_datetime = parts[2]\n",
        "\n",
        "    # Extract date from the start_datetime (assuming the format is like '20180601T014926')\n",
        "    sensing_date = start_datetime[:8]\n",
        "    start_time = start_datetime[9:]\n",
        "\n",
        "    return satellite, sensing_date, start_time\n",
        "\n",
        "def get_access_token(username, password):\n",
        "    \"\"\"\n",
        "    Retrieves access token from Copernicus Dataspace using the provided credentials.\n",
        "\n",
        "    Parameters:\n",
        "    username (str): Username for Copernicus Dataspace.\n",
        "    password (str): Password for Copernicus Dataspace.\n",
        "\n",
        "    Returns:\n",
        "    str: Access token for authenticated sessions.\n",
        "    \"\"\"\n",
        "    url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
        "    data = {\n",
        "        'grant_type': 'password',\n",
        "        'username': username,\n",
        "        'password': password,\n",
        "        'client_id': 'cdse-public'\n",
        "    }\n",
        "    response = requests.post(url, data=data)\n",
        "    response.raise_for_status()\n",
        "    return response.json()['access_token']\n",
        "\n",
        "def query_sentinel2_data(sensing_start_date, tile_number, token):\n",
        "    \"\"\"\n",
        "    Queries the Sentinel-2 data from the Copernicus Data Space based on sensing start date, tile number, and access token.\n",
        "\n",
        "    Parameters:\n",
        "    sensing_start_date (str): The start date and time for the data sensing in the format 'YYYYMMDDTHHMMSS'.\n",
        "    tile_number (str): The specific tile number of the Sentinel-2 data to be queried.\n",
        "    token (str): The access token for authenticating requests to the Copernicus Data Space.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame containing the query results with details about the Sentinel-2 data.\n",
        "\n",
        "    The function constructs a query URL with specified parameters, sends a request to the Copernicus Data Space,\n",
        "    and returns the results as a DataFrame. It filters the data based on the tile number and the content start date\n",
        "    within a certain time window.\n",
        "    \"\"\"\n",
        "    # Convert sensing_start_date to datetime object and format it for the query\n",
        "    start_time = datetime.strptime(sensing_start_date, '%Y%m%dT%H%M%S')\n",
        "    end_time = start_time + timedelta(hours=2)  # Adjust the time window as necessary\n",
        "    start_time_str = start_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    end_time_str = end_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "    # Construct the request URL with the contains function for tile number\n",
        "    url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(Name,'{tile_number}') and Collection/Name eq 'SENTINEL-2' and ContentDate/Start gt {start_time_str} and ContentDate/Start lt {end_time_str}\"\n",
        "    headers = {'Authorization': f'Bearer {token}'}\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return pd.DataFrame.from_dict(response.json()['value'])\n",
        "\n",
        "def query_sentinel3_olci_data(satellite, sensing_date, start_time, token):\n",
        "    \"\"\"\n",
        "    Queries Sentinel-3 OLCI data from Copernicus Data Space based on satellite name, sensing date, and start time.\n",
        "\n",
        "    Parameters:\n",
        "    satellite (str): Name of the satellite.\n",
        "    sensing_date (str): Date of the data sensing.\n",
        "    start_time (str): Start time of the data sensing.\n",
        "    token (str): Access token for authentication.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame containing the query results with details about the Sentinel-3 OLCI data.\n",
        "    \"\"\"\n",
        "    # Convert sensing_date to datetime object and format it for the query\n",
        "    sensing_datetime = datetime.strptime(f'{sensing_date}T{start_time}', '%Y%m%dT%H%M%S')\n",
        "    sensing_datetime = sensing_datetime - timedelta(seconds=1)\n",
        "\n",
        "    # Construct the request URL using the filter structure provided\n",
        "    url = (\n",
        "        f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?\"\n",
        "        f\"$filter=contains(Name,'{satellite}') and \"\n",
        "        f\"ContentDate/Start ge {sensing_datetime.strftime('%Y-%m-%dT%H:%M:%S.000Z')} and \"\n",
        "        f\"ContentDate/Start le {(sensing_datetime + timedelta(days=1)).strftime('%Y-%m-%dT%H:%M:%S.000Z')}&\"\n",
        "        f\"$orderby=ContentDate/Start&$top=1000\"\n",
        "    )\n",
        "    headers = {'Authorization': f'Bearer {token}'}\n",
        "\n",
        "    # Print the URL for debugging\n",
        "    print(url)\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.get(url, headers=headers)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        # Print error details and return an empty DataFrame if the request failed\n",
        "        print(f\"Error: Unable to fetch data. Status Code: {response.status_code}. Response: {response.text}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert the JSON response to a DataFrame\n",
        "    search_results_df = pd.DataFrame.from_dict(response.json()['value'])\n",
        "\n",
        "    # Convert the 'ContentDate/Start' to datetime objects and sort the results\n",
        "    search_results_df['SensingStart'] = pd.to_datetime(search_results_df['ContentDate'].apply(lambda x: x['Start']))\n",
        "    search_results_df.sort_values(by='SensingStart', inplace=True)\n",
        "\n",
        "    return search_results_df\n",
        "\n",
        "def extract_correct_product_name(df, start_time, tile_number):\n",
        "    \"\"\"\n",
        "    Extracts the correct product name and ID from a dataframe based on a specific start time and tile number.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): The dataframe containing product information.\n",
        "    start_time (str): The start time used to filter the products.\n",
        "    tile_number (str): The tile number used to filter the products.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the first matching product name and ID, or (None, None) if no match is found.\n",
        "    \"\"\"\n",
        "    # Adjusted regex pattern to match the filename format\n",
        "    pattern = f'MSIL1C.*{start_time}.*_{tile_number}_'\n",
        "    filtered_products = df[df['Name'].str.contains(pattern, regex=True)]\n",
        "\n",
        "\n",
        "    # Return the first matching product name, or None if not found\n",
        "    return filtered_products['Name'].iloc[0] if not filtered_products.empty else None, filtered_products['Id'].iloc[0] if not filtered_products.empty else None\n",
        "\n",
        "def process_image_pair(s2_ee_image_id, token):\n",
        "    \"\"\"\n",
        "    Processes a pair of Sentinel-2 images by querying the Copernicus Data Space to find the corresponding product name and ID.\n",
        "\n",
        "    Parameters:\n",
        "    s2_ee_image_id (str): The Sentinel-2 Earth Engine image ID.\n",
        "    token (str): The access token for authenticating requests to the Copernicus Data Space.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the product name and ID for the corresponding Sentinel-2 image.\n",
        "    \"\"\"\n",
        "    sensing_start_date = s2_ee_image_id.split('_')[0]\n",
        "    tile_number = s2_ee_image_id.split('_')[2]\n",
        "\n",
        "    # Query the Copernicus Data Space\n",
        "    df = query_sentinel2_data(sensing_start_date, tile_number, token)\n",
        "\n",
        "    # Extract the correct MSIL1C product name\n",
        "    return extract_correct_product_name(df, sensing_start_date, tile_number)\n",
        "\n",
        "def fetch_S3_images_by_area_and_date(date_range, spatial_extent, area_of_interest):\n",
        "    \"\"\"\n",
        "    Fetches Sentinel-3 OLCI images based on a specified date range and area of interest.\n",
        "\n",
        "    :param date_range: List containing the start and end dates (e.g., ['2018-06-01', '2018-06-02'])\n",
        "    :param spatial_extent: List containing the spatial extent [min_lon, min_lat, max_lon, max_lat]\n",
        "    :param area_of_interest: ee.Geometry object defining the specific area for which to fetch images\n",
        "\n",
        "    :return: List of dictionaries, each containing details about a fetched image, including its ID, date, and download URL.\n",
        "    \"\"\"\n",
        "    # Initialize the Earth Engine module\n",
        "    ee.Initialize()\n",
        "\n",
        "    # Define variables for Sentinel-3 OLCI query\n",
        "    S3_product = 'COPERNICUS/S3/OLCI'\n",
        "\n",
        "    # Query for Sentinel-3 data within the specified date range and area of interest\n",
        "    S3_collection = ee.ImageCollection(S3_product) \\\n",
        "        .filterDate(date_range[0], date_range[1]) \\\n",
        "        .filterBounds(area_of_interest)\n",
        "\n",
        "    # Convert S3_collection to a list of image IDs\n",
        "    S3_image_ids = S3_collection.aggregate_array('system:index').getInfo()\n",
        "    S3_images_info = S3_collection.getInfo()['features']\n",
        "\n",
        "    # Initialize an empty list to store details\n",
        "    S3_image_details = []\n",
        "\n",
        "    # Iterate through each image in the collection\n",
        "    for img_info in S3_images_info:\n",
        "        # Fetch image ID\n",
        "        image_id = img_info['id']\n",
        "\n",
        "        # Fetch image date and other properties as needed\n",
        "        image_date = img_info['properties']['system:time_start']  # Example property\n",
        "\n",
        "        # Append the details to the list\n",
        "        S3_image_details.append({\n",
        "            'id': image_id,\n",
        "            'date': image_date\n",
        "        })\n",
        "\n",
        "    return S3_image_details\n",
        "\n",
        "def download_single_product(product_id, file_name, access_token, download_dir=\"downloaded_products\"):\n",
        "    \"\"\"\n",
        "    Download a single product from the Copernicus Data Space.\n",
        "\n",
        "    :param product_id: The unique identifier for the product.\n",
        "    :param file_name: The name of the file to be downloaded.\n",
        "    :param access_token: The access token for authorization.\n",
        "    :param download_dir: The directory where the product will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the download directory exists\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    # Construct the download URL\n",
        "    url = f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({product_id})/$value\"\n",
        "\n",
        "    # Set up the session and headers\n",
        "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    # Perform the request\n",
        "    response = session.get(url, headers=headers, stream=True)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Define the path for the output file\n",
        "        output_file_path = os.path.join(download_dir, file_name + \".zip\")\n",
        "\n",
        "        # Stream the content to a file\n",
        "        with open(output_file_path, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    file.write(chunk)\n",
        "        print(f\"Downloaded: {output_file_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download product {product_id}. Status Code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentinel-2 Optical Image\n",
        "The data S2 data can be visualised in the [Copernicus Browser](https://browser.dataspace.copernicus.eu/?zoom=8&lat=74.67164&lng=-68.05701&themeId=DEFAULT-THEME&visualizationUrl=https%3A%2F%2Fsh.dataspace.copernicus.eu%2Fogc%2Fwms%2Fa91f72b5-f393-4320-bc0f-990129bd9e63&datasetId=S2_L2A_CDAS&fromTime=2024-03-04T00%3A00%3A00.000Z&toTime=2024-03-04T23%3A59%3A59.999Z&layerId=1_TRUE_COLOR&demSource3D=%22MAPZEN%22&cloudCoverage=30&dateMode=SINGLE).\n",
        "Be sure to change the gee_filename to the one you desire. In this project, we are looking at the ocean betwen Greenland and Canada on the date (04/03/2024). Note that the sensing time is at 171211."
      ],
      "metadata": {
        "id": "gzZehawlKLdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gee_filename = '20240304T171211_20240304T203849_T19XEC'           # change accordingly\n",
        "file_name, product_id = process_image_pair(gee_filename, token)\n",
        "\n",
        "# Get access token for Copernicus Data Space (ensure your credentials are correct)\n",
        "username = 'affankb1317@gmail.com'      # be sure to enter your own credentials\n",
        "password = '4ffanMazlan_'\n",
        "token = get_access_token(username, password)\n",
        "access_token = token\n",
        "\n",
        "print(file_name)\n",
        "print(product_id)\n",
        "\n",
        "download_dir = '/content/drive/MyDrive/GEOL0069/EOYA'     # replace with your directory\n",
        "download_single_product(product_id, file_name, access_token, download_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbRu8vxjEbIN",
        "outputId": "e2a7aadb-5293-4e7e-a53e-974223c9389c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S2A_MSIL1C_20240304T171211_N0510_R112_T19XEC_20240304T192251.SAFE\n",
            "e3fe8a77-3fa4-40e4-937f-2b925d3449b2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colocated Sentinel-3 OLCI data\n",
        "With the acquired Sentinel-2 data, the Sentinel-3 OLCI data is obtained for the same date (04/03/2024), just under two hours apart (155608)."
      ],
      "metadata": {
        "id": "Y9CRhA2KoBNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gee_image_id = 'S3B_20240304T155608_20240304T155711'\n",
        "\n",
        "# Parse the GEE filename to get the date and time\n",
        "satellite, sensing_date, start_time = parse_gee_filename_s3(gee_image_id)\n",
        "\n",
        "# Get access token for Copernicus Data Space (ensure your credentials are correct)\n",
        "username ='affankb1317@gmail.com'\n",
        "password ='4ffanMazlan_'\n",
        "token = get_access_token(username, password)\n",
        "\n",
        "# Query the Copernicus Data Space for the corresponding Sentinel-3 OLCI data\n",
        "s3_olci_data = query_sentinel3_olci_data(satellite, sensing_date, start_time, token)\n",
        "\n",
        "# Assuming s3_olci_data is a DataFrame or a dictionary containing the query results\n",
        "# Print the first filename and id from the list, which is usually the one we want\n",
        "print(s3_olci_data['Name'][0])\n",
        "print(s3_olci_data['Id'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39l7q2yFiLcV",
        "outputId": "e07aaaac-8446-4537-e8a7-642776fc782c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(Name,'S3B_OL_1_EFR') and ContentDate/Start ge 2024-03-04T15:56:07.000Z and ContentDate/Start le 2024-03-05T15:56:07.000Z&$orderby=ContentDate/Start&$top=1000\n",
            "S3B_OL_1_EFR____20240304T155608_20240304T155711_20240305T084636_0063_090_211_1620_PS2_O_NT_003.SEN3\n",
            "af0704af-c08f-428e-9bc5-b93735db2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_dir = \"/content/drive/MyDrive/GEOL0069/EOYA\"  # Replace with your desired download directory\n",
        "product_id = s3_olci_data['Id'][0]\n",
        "file_name = s3_olci_data['Name'][0]\n",
        "# Download the single product\n",
        "download_single_product(product_id, file_name, access_token, download_dir)"
      ],
      "metadata": {
        "id": "s-ysI27qinqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentinel-3 Altimetry Data\n",
        "The next set of functions is to find the altimetry data associated with the Sentinel-3 OLCI data obtained from the above steps."
      ],
      "metadata": {
        "id": "M3wGTUxD9r6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUa_Jh0yOYm9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import json\n",
        "from datetime import date\n",
        "from joblib import Parallel, delayed\n",
        "import zipfile\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "def get_access_token(username, password):\n",
        "    \"\"\"\n",
        "    Obtain an access token to the Copernicus Data Space Ecosystem.\n",
        "    Necessary for the download of hosted products.\n",
        "    \"\"\"\n",
        "    p =  subprocess.run(f\"curl --location --request POST 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' \\\n",
        "            --header 'Content-Type: application/x-www-form-urlencoded' \\\n",
        "            --data-urlencode 'grant_type=password' \\\n",
        "            --data-urlencode 'username={username}' \\\n",
        "            --data-urlencode 'password={password}' \\\n",
        "            --data-urlencode 'client_id=cdse-public'\", shell=True,capture_output=True, text=True)\n",
        "    access_dict = json.loads(p.stdout)\n",
        "    return access_dict['access_token'], access_dict['refresh_token']\n",
        "\n",
        "#=============================================================================================================================================================#\n",
        "\n",
        "def get_new_access_token(refresh_token):\n",
        "    \"\"\"\n",
        "    Obtain a new access token to the Copernicus Data Space Ecosystem using a previously provided refesh token.\n",
        "    \"\"\"\n",
        "    p =  subprocess.run(f\"curl --location --request POST 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' \\\n",
        "    --header 'Content-Type: application/x-www-form-urlencoded' \\\n",
        "    --data-urlencode 'grant_type=refresh_token' \\\n",
        "    --data-urlencode 'refresh_token={refresh_token}' \\\n",
        "    --data-urlencode 'client_id=cdse-public'\", shell=True,capture_output=True, text=True)\n",
        "    access_dict = json.loads(p.stdout)\n",
        "    return access_dict['access_token'], access_dict['refresh_token']\n",
        "\n",
        "#=============================================================================================================================================================#\n",
        "\n",
        "def get_S3_SI_search_results_df(date):\n",
        "    \"\"\"\n",
        "    Obtain a pandas dataframe of Sentinel-3 sea ice thematic products for a given date.\n",
        "    \"\"\"\n",
        "    json = requests.get(f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-3' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'SR_2_LAN_SI') and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'timeliness' and att/OData.CSC.StringAttribute/Value eq 'NT') and ContentDate/Start gt {(date-pd.Timedelta(days=1)).strftime('%Y-%m-%dT%H:%M:%SZ')} and ContentDate/End lt {(date+pd.Timedelta(days=2)).strftime('%Y-%m-%dT%H:%M:%SZ')}&$top=1000\").json()\n",
        "\n",
        "    results_df = pd.DataFrame.from_dict(json['value'])\n",
        "    results_df['Satellite'] = [row['Name'][:3] for i,row in results_df.iterrows()]\n",
        "    results_df['SensingStart'] = [pd.to_datetime(row['ContentDate']['Start']) for i,row in results_df.iterrows()]\n",
        "    results_df['SensingEnd'] = [pd.to_datetime(row['ContentDate']['End']) for i,row in results_df.iterrows()]\n",
        "    results_df =  results_df[(results_df['SensingEnd'] >= date) & (results_df['SensingStart'] <= date+pd.Timedelta(days=1))]\n",
        "    results_df = results_df.sort_values(by='SensingStart')\n",
        "    return results_df\n",
        "\n",
        "#=============================================================================================================================================================#\n",
        "\n",
        "def filter_duplicate_products_versions(results_df, keep_latest=True ):\n",
        "    \"\"\"\n",
        "    Filter Sentinel-3 product dataframe to remove duplicate verions of files.\n",
        "    By default, we keep the latest version of the file. E.g., where an operation version\n",
        "    and a reprocessed version exists, we keep the reprocessed version.\n",
        "    \"\"\"\n",
        "    results_df['name_snippet'] = [row['Name'][:47] for i,row in results_df.iterrows()]\n",
        "    if  keep_latest == True:\n",
        "        keep='last'\n",
        "    else:\n",
        "        keep = 'first'\n",
        "\n",
        "    results_df = (\n",
        "        results_df\n",
        "        .sort_values(by='ModificationDate')\n",
        "        .drop_duplicates(subset=['name_snippet'], keep=keep)\n",
        "        .drop(columns = ['name_snippet'])\n",
        "        .sort_values(by='SensingStart')\n",
        "    )\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def find_overlapping_sar(olci_filename, search_results_df):\n",
        "    # Extract date and time from OLCI filename\n",
        "    parts = olci_filename.split('_')\n",
        "    olci_date_time = datetime.strptime(parts[7], '%Y%m%dT%H%M%S')\n",
        "\n",
        "    # Filter SAR filenames based on overlapping criteria\n",
        "    # This is a placeholder logic, adjust according to your specific criteria\n",
        "    overlapping_sar = search_results_df[search_results_df['Name'].apply(lambda x: 'S3' in x and 'SR_2_LAN_SI' in x)]\n",
        "\n",
        "    return overlapping_sar\n",
        "\n",
        "\n",
        "def get_date_from_olci_filename(olci_filename):\n",
        "    \"\"\"\n",
        "    Extracts the date from an OLCI filename.\n",
        "\n",
        "    Parameters:\n",
        "    olci_filename (str): The OLCI filename.\n",
        "\n",
        "    Returns:\n",
        "    datetime.date: The date extracted from the filename.\n",
        "    \"\"\"\n",
        "    parts = olci_filename.split('_')\n",
        "    date_str = parts[7][:8]  # Extract date part and truncate to YYYYMMDD format\n",
        "    return pd.to_datetime(date_str, format='%Y%m%d').date()\n",
        "\n",
        "def get_overlapping_sar_file(olci_filename, get_S3_SI_search_results_df, token):\n",
        "    olci_date = get_date_from_olci_filename(olci_filename)\n",
        "    start_date = olci_date - pd.Timedelta(days=1)\n",
        "    end_date = olci_date + pd.Timedelta(days=1)\n",
        "    dates = pd.date_range(start_date, end_date)\n",
        "\n",
        "    all_overlapping_sar = pd.DataFrame()  # Collect all overlapping SAR files\n",
        "\n",
        "    for date in dates:\n",
        "        date = date.tz_localize('UTC')\n",
        "        search_results_df = get_S3_SI_search_results_df(date)\n",
        "\n",
        "        if search_results_df.empty:\n",
        "            print(f\"No SAR data found for date: {date}\")\n",
        "            continue\n",
        "\n",
        "        filtered_df = filter_duplicate_products_versions(search_results_df)\n",
        "        if filtered_df.empty:\n",
        "            print(f\"No SAR data after filtering for date: {date}\")\n",
        "            continue\n",
        "\n",
        "        overlapping_sar = find_overlapping_sar(olci_filename, filtered_df)\n",
        "        if not overlapping_sar.empty:\n",
        "            all_overlapping_sar = pd.concat([all_overlapping_sar, overlapping_sar], ignore_index=True)\n",
        "\n",
        "    return all_overlapping_sar\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def check_overlap(row, olci_filename, olci_start, olci_end):\n",
        "    \"\"\"\n",
        "    Checks if the SAR file's sensing period overlaps with the OLCI file's sensing period and if it's from the same satellite.\n",
        "\n",
        "    Parameters:\n",
        "    row (Series): A row from the SAR search results DataFrame.\n",
        "    olci_filename (str): The OLCI filename.\n",
        "    olci_start (datetime): Start time of OLCI sensing period.\n",
        "    olci_end (datetime): End time of OLCI sensing period.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if there's an overlap and the satellite is consistent, False otherwise.\n",
        "    \"\"\"\n",
        "    # Extract satellite identifier from the OLCI filename\n",
        "    satellite = olci_filename.split('_')[0]  # e.g., S3A or S3B\n",
        "\n",
        "    # Parse SAR start and end times\n",
        "    sar_start = datetime.strptime(row['ContentDate']['Start'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    sar_end = datetime.strptime(row['ContentDate']['End'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "\n",
        "    # Check for temporal overlap and satellite consistency\n",
        "    is_temporal_overlap = sar_start <= olci_end and sar_end >= olci_start\n",
        "    is_same_satellite = satellite in row['Name']\n",
        "\n",
        "    return is_temporal_overlap and is_same_satellite\n",
        "\n",
        "# Adjust the find_overlapping_sar function to include the OLCI filename in the check_overlap call\n",
        "def find_overlapping_sar(olci_filename, search_results_df):\n",
        "    # Extract date and time from OLCI filename\n",
        "    parts = olci_filename.split('_')\n",
        "    olci_sensing_start = datetime.strptime(parts[7], '%Y%m%dT%H%M%S')\n",
        "    olci_sensing_end = datetime.strptime(parts[8], '%Y%m%dT%H%M%S')\n",
        "\n",
        "    # Filter for SAR files that overlap with the OLCI sensing period\n",
        "    overlapping_sar = search_results_df[search_results_df.apply(lambda row: check_overlap(row, olci_filename, olci_sensing_start, olci_sensing_end), axis=1)]\n",
        "\n",
        "    return overlapping_sar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Sentinel 3 Altimetry Data"
      ],
      "metadata": {
        "id": "pSIIRnfP-2y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username ='affankb1317@gmail.com'\n",
        "password ='4ffanMazlan_'\n",
        "\n",
        "token = get_access_token(username, password)\n",
        "token, refresh_token = get_access_token(username, password)\n",
        "\n",
        "olci_filename = 'S3B_OL_1_EFR____20240304T155608_20240304T155711_20240305T084636_0063_090_211_1620_PS2_O_NT_003.SEN3' # replace with the one you are interested in.\n",
        "overlapped_df = get_overlapping_sar_file(olci_filename, get_S3_SI_search_results_df, token)\n",
        "product_id = overlapped_df['Id'].iloc[0]\n",
        "file_name = overlapped_df['Name'].iloc[0]\n",
        "download_dir = '/content/drive/MyDrive/GEOL0069/EOYA'\n",
        "download_single_product(product_id, file_name, token, download_dir)  # altimetry data"
      ],
      "metadata": {
        "id": "A51_QPAN9wdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloaded products\n",
        "Now, we have three different folders downloaded in this notebook, which includes:\n",
        "1.   Sentinel-2 Optical Image\n",
        "2.   Colocated Sentinel-3 OLCI Data\n",
        "3.   Corresponding Sentinel-3 Altimetry Data\n",
        "\n"
      ],
      "metadata": {
        "id": "vcVt5iU7vRIv"
      }
    }
  ]
}